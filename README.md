# How to evaluate and monitor LLM deployments in real-time?

This is a repository for my work about the experiments with the Evaluator Library and the Supervisory Multi-Agent System. Data and scripts for the former are in the discrimination_results folder and files for the agent system are in the supervisory_agent folder, both folders include an extra README. Final prompts for evaluators and prompt variations for the experiments are in the prompts folder - Discrimination.pdf has prompt variations for the LLM-as-a-judge experiment, Final evals.pdf has the final prompts included in the evaluation library, and Evaluator Generator.pdf has the Evaluator Generator meta prompt.
 
